const cron = require('node-cron');
const { scrapeLinkedInJobs, saveJobsToSupabase: saveLinkedInJobs, CONFIG: linkedInConfig } = require('./scraper');
const { scrapeGlassdoorJobs, saveJobsToSupabase: saveGlassdoorJobs, CONFIG: glassdoorConfig } = require('./glassdoor');
const { scrapeReedJobs, saveJobsToSupabase: saveReedJobs, CONFIG: reedConfig } = require('./reed');

const isRunNow = process.argv.includes('--run-now');

/**
 * Run LinkedIn scraper
 */
async function runLinkedInScraper() {
    console.log('\nüîµ Starting LinkedIn scraper...');
    console.log(`‚è∞ Time: ${new Date().toISOString()}`);

    try {
        const jobs = await scrapeLinkedInJobs({
            keywords: process.env.SCRAPE_KEYWORDS || linkedInConfig.keywords,
            location: process.env.SCRAPE_LOCATION || linkedInConfig.location,
            maxJobs: parseInt(process.env.SCRAPE_MAX_JOBS) || linkedInConfig.maxJobs,
        });

        await saveLinkedInJobs(jobs);
        console.log('‚úÖ LinkedIn scraping complete!');
    } catch (error) {
        console.error('‚ùå LinkedIn scraper failed:', error);
    }
}

/**
 * Run Glassdoor scraper
 */
async function runGlassdoorScraper() {
    console.log('\nüü¢ Starting Glassdoor scraper...');
    console.log(`‚è∞ Time: ${new Date().toISOString()}`);

    try {
        const jobs = await scrapeGlassdoorJobs({
            keywords: process.env.SCRAPE_KEYWORDS || glassdoorConfig.keywords,
            location: process.env.SCRAPE_LOCATION || glassdoorConfig.location,
            maxJobs: parseInt(process.env.SCRAPE_MAX_JOBS) || glassdoorConfig.maxJobs,
        });

        await saveGlassdoorJobs(jobs);
        console.log('‚úÖ Glassdoor scraping complete!');
    } catch (error) {
        console.error('‚ùå Glassdoor scraper failed:', error);
    }
}

/**
 * Run Reed scraper (visa sponsorship jobs)
 */
async function runReedScraper() {
    console.log('\nüü† Starting Reed scraper (visa sponsorship)...');
    console.log(`‚è∞ Time: ${new Date().toISOString()}`);

    try {
        const jobs = await scrapeReedJobs({
            keywords: process.env.REED_KEYWORDS || reedConfig.keywords,
            location: process.env.REED_LOCATION || reedConfig.location,
            maxJobs: parseInt(process.env.REED_MAX_JOBS) || reedConfig.maxJobs,
        });

        await saveReedJobs(jobs);
        console.log('‚úÖ Reed scraping complete!');
    } catch (error) {
        console.error('‚ùå Reed scraper failed:', error);
    }
}

/**
 * Run all scrapers
 */
async function runAllScrapers() {
    console.log('\nüöÄ Running all job scrapers...');
    console.log('================================\n');

    await runLinkedInScraper();
    await runGlassdoorScraper();
    await runReedScraper();

    console.log('\n================================');
    console.log('‚úÖ All scrapers complete!\n');
}

// Run immediately if --run-now flag is passed
if (isRunNow) {
    console.log('üîß Running scrapers immediately (--run-now flag detected)');
    runAllScrapers().then(() => {
        console.log('Done!');
        process.exit(0);
    });
} else {
    // Schedule to run every 6 hours
    console.log('üìÖ Scheduling scrapers to run every 6 hours...');
    console.log('   Sources: LinkedIn, Glassdoor, Reed (visa sponsorship)');
    console.log('   (Use "npm run scrape" to run immediately)\n');

    cron.schedule('0 */6 * * *', () => {
        runAllScrapers();
    });

    // Keep process alive
    console.log('‚è≥ Scrapers waiting for scheduled runs...');
}
